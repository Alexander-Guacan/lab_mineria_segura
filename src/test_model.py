import os
import joblib
import numpy as np
import pandas as pd
from extract_features import extract_metrics_from_file
import json

MODEL_PATH = "data/meta/best_model.pkl"
SCALER_PATH = "data/meta/scaler.pkl"
METADATA_PATH = "data/results/model_metadata.json"

def test_model_on_file(filepath):
    """Prueba el modelo en un archivo espec√≠fico"""
    
    print("=" * 70)
    print(f"PROBANDO MODELO EN: {filepath}")
    print("=" * 70)
    
    # Cargar modelo y scaler
    model = joblib.load(MODEL_PATH)
    scaler = joblib.load(SCALER_PATH) if os.path.exists(SCALER_PATH) else None
    
    # Cargar metadata
    with open(METADATA_PATH, 'r') as f:
        metadata = json.load(f)
    
    print(f"\nüìä Informaci√≥n del modelo:")
    print(f"   Tipo: {metadata['model_type']}")
    print(f"   Precisi√≥n (CV): {metadata['cv_accuracy']:.2%}")
    print(f"   Precisi√≥n (Test): {metadata['test_accuracy']:.2%}")
    print(f"   F1-Score: {metadata['test_f1']:.2%}")
    print(f"   AUC: {metadata['test_auc']:.2%}")
    
    # Extraer features del archivo
    print(f"\nüîç Extrayendo caracter√≠sticas del archivo...")
    metrics = extract_metrics_from_file(filepath)
    
    if metrics is None:
        print("‚ùå Error al extraer m√©tricas")
        return
    
    # Agregar size_bytes
    metrics["size_bytes"] = os.path.getsize(filepath)
    
    # Mostrar m√©tricas extra√≠das
    print(f"\nüìà Caracter√≠sticas extra√≠das:")
    for key, value in metrics.items():
        print(f"   {key}: {value}")
    
    # Preparar datos para predicci√≥n
    features = metadata['basic_features']
    
    
    
    X = pd.DataFrame(
        [[metrics.get(feat, 0) for feat in features]], 
        columns=features
    )
    
    # Aplicar scaler
    if scaler:
        X_scaled = scaler.transform(X)
        X = pd.DataFrame(X_scaled, columns=features)
    
    # Predecir
    prediction = model.predict(X)[0]
    probability = model.predict_proba(X)[0]
    
    print(f"\n{'='*70}")
    print("RESULTADO DE LA PREDICCI√ìN")
    print(f"{'='*70}")
    print(f"\nüéØ Clasificaci√≥n: {'‚ö†Ô∏è VULNERABLE' if prediction == 1 else '‚úÖ SEGURO'}")
    print(f"üìä Probabilidad de ser SEGURO: {probability[0]:.2%}")
    print(f"üìä Probabilidad de ser VULNERABLE: {probability[1]:.2%}")
    
    # Interpretaci√≥n
    print(f"\nüí° Interpretaci√≥n:")
    if probability[1] >= 0.8:
        print("   üî¥ ALTO RIESGO - Revisar urgentemente")
    elif probability[1] >= 0.5:
        print("   üü° RIESGO MEDIO - Se recomienda revisi√≥n")
    else:
        print("   üü¢ BAJO RIESGO - C√≥digo relativamente seguro")
    
    return {
        "prediction": int(prediction),
        "probability_safe": float(probability[0]),
        "probability_vulnerable": float(probability[1]),
        "metrics": metrics
    }


def test_multiple_files(filepaths):
    """Prueba el modelo en m√∫ltiples archivos"""
    results = []
    
    for filepath in filepaths:
        if os.path.exists(filepath):
            result = test_model_on_file(filepath)
            if result:
                result['filepath'] = filepath
                results.append(result)
        else:
            print(f"‚ùå Archivo no encontrado: {filepath}")
    
    # Resumen
    print(f"\n{'='*70}")
    print("RESUMEN DE RESULTADOS")
    print(f"{'='*70}")
    
    for result in results:
        status = "‚ö†Ô∏è VULNERABLE" if result['prediction'] == 1 else "‚úÖ SEGURO"
        prob = result['probability_vulnerable']
        print(f"{status} | {prob:.1%} | {result['filepath']}")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("Uso: python test_model.py <archivo.py> [archivo2.py ...]")
        print("\nEjemplo:")
        print("  python test_model.py ..\\tests\\vulnerable_code.py")
        print("  python test_model.py ..\\tests\\vulnerable_code.py ..\\tests\\flask_project\\app.py")
    else:
        test_multiple_files(sys.argv[1:])